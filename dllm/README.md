<h1 align="center">dLLM</h1>

<p align="center">
Training Diffusion Large Language Models Made Simple
</p>

<p align="center">
<img
  src="assets/logo.gif"
  alt="dLLM logo">
</p>


## Overview
**dLLM** is an educational library offering unified implementations for training **diffusion language models**. It brings transparency to the entire training and deployment process, making **reproduction and finetuning** of open-weight diffusion language models much easier. Below are some of the key features that make dLLM special:

- dLLM provides reproduction and finetuning recipes for a variety of open-weight models (e.g., [LLaDA](https://arxiv.org/abs/2502.09992), [Dream](https://arxiv.org/abs/2508.15487) and [RND1](https://www.radicalnumerics.ai/assets/rnd1_report.pdf)), and provides reference implementation of various training algorithms (e.g., [Edit Flows](https://arxiv.org/abs/2506.09018)).

- dLLM, built on top of [ðŸ¤— Transformers](https://github.com/huggingface/transformers), scales seamlesslyâ€”from edge devices with [LoRA](https://github.com/huggingface/peft) to multi-node clusters with [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and beyond.

- dLLM provides unified, modular training pipelines (inspired by [ðŸ¤— Transformers Trainer](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py)) and well-documented [examples](/examples/), making customization simple and development highly user-friendly.

> [!NOTE]
> This repository is primarily for educational purposes and does not aim for 100% exact reproduction of official models (which is impossible). We hope it serves as a helpful reference for the community â€” contributions and improvements are always welcome!


## Table of Contents
<!-- - [Overview](#overview) -->
- [Features & Documentation](#features--documentations)
- [Setup](#setup)
  <!-- - [Installation](#installation)
  - [(optional) Slurm setup](#optional-slurm-setup) -->
- [Files overview](#files-overview)
- [Training](#training)
- [Roadmap](#roadmap)
- [Citation](#citation)


## Features & Documentations

1. [`examples/llada`](/examples/llada): Finetuning open-weight LLaDA [LLaDA](https://arxiv.org/abs/2502.09992) / [LLaDA-MoE](https://arxiv.org/abs/2509.24389), as well as reproducing LLaDA by training from scratch on public data (pretraining & finetuning).
2. [`examples/dream`](/examples/dream): Finetuning open-weight Dream [Dream](https://arxiv.org/abs/2508.15487), as well as reproducing Dream by training from scratch on public data (pretraining & finetuning).
3. [`examples/rnd`](/examples/rnd): (WIP) Finetuning open-weight RND1 [RND1-Base](https://www.radicalnumerics.ai/assets/rnd1_report.pdf).
4. [`examples/editflow`](/examples/editflow): Educational reference for training [EditFlow](https://arxiv.org/abs/2506.09018) models, demonstrating how to extend existing DLLMs (e.g., LLaDA and Dream) with *edit operations*â€”insertion, deletion, and substitutionâ€”and how to pretrain or finetune EditFlow models from scratch on public data.

   <details>
   <summary>ðŸŽ¬ Click to show EditFlow Demo</summary>

   <p align="center">
     <img src="/examples/editflow/assets/all.gif" alt="EditFlow demo" width="100%">
   </p>
   <p align="center"><em>EditFlow performing insertion (blue), substitution from mask tokens (black), substitution from non-mask tokens (red), and deletion (strikethrough â†’ removed) during generation.</em></p>

   </details>


4. More upcoming â€” see [Roadmap](#roadmap).


## Setup
### Installation
```bash
# create and activate conda environment
conda create -n dllm python=3.10 -y
conda activate dllm

# install pytorch with CUDA 12.4 (other pytorch/cuda versions should also work)
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
    --index-url https://download.pytorch.org/whl/cu124

# install requirements
pip install -r requirements.txt

# install dllm package
pip install -e .
```
### (optional) Slurm setup
For [Slurm](https://slurm.schedmd.com/) users, update [`scripts/train.slurm.sh`](/scripts/train.slurm.sh) for your cluster:
```diff
- #SBATCH --partition=mllm_safety # Note: adjust this for your cluster
- #SBATCH --quotatype=spot        # Note: adjust this for your cluster
+ #SBATCH --partition=YOUR_PARTITION
+ #SBATCH --quotatype=YOUR_QUOTATYPE
```
Next, create a directory for your job logs:
```shell
mkdir logs
```
This folder will store the log files generated by your sbatch jobs.

## Files overview
```
# modules for training / sampling
dllm
â”œâ”€â”€ core                # Core reusable modules shared across `dllm/pipelines` 
â”‚   â”œâ”€â”€ schedulers
â”‚   â””â”€â”€ trainers
â”œâ”€â”€ data
â”œâ”€â”€ pipelines           # Application-specific training & inference pipelines
â”‚   â”œâ”€â”€ dream
â”‚   â”œâ”€â”€ editflow
â”‚   â””â”€â”€ llada
â”‚       â”œâ”€â”€ models      # Model architecture and configs 
â”‚       â”œâ”€â”€ generate.py # Generation utilities
â”‚       â””â”€â”€ trainer.py  # Core training logic
â”œâ”€â”€ tools
â””â”€â”€ utils

# entry points for training / sampling
examples
â”œâ”€â”€ dream
â”œâ”€â”€ editflow
â””â”€â”€ llada
    â”œâ”€â”€ generate.py    # Generation example
    â”œâ”€â”€ pt.py          # Pretraining example
    â”œâ”€â”€ README.md      # Example-level documentations
    â””â”€â”€ sft.py         # SFT example
```

A typical training entry script look like (for example, [`examples/llada/sft.py`](/examples/llada/sft.py)) looks like this:
```python
import transformers

import dllm

model_args, data_args, training_args = parser.parse_args_into_dataclasses()
# ----- Model ------------------------------------------------------------------
model = dllm.utils.get_model(model_args=model_args)
# ----- Tokenizer --------------------------------------------------------------
tokenizer = dllm.utils.get_tokenizer(model=model, model_args=model_args)
# ----- Dataset ----------------------------------------------------------------
dataset = "..."

# ----- Training --------------------------------------------------------------
trainer = dllm.pipelines.llada.LLaDATrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    data_collator=transformers.DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True,
        label_pad_token_id=tokenizer.pad_token_id,  # LLaDA is trained on padding <eos_token>
    ),
)
trainer.train()
```

## Training
You can launch training job locally with `accelerate`, or submit it to a [Slurm](https://slurm.schedmd.com/) cluster using `sbatch`.
```shell
# Run locally (DeepSpeed ZeRO-2 with 8 GPUs)
accelerate launch \
    --config_file scripts/accelerate_configs/deepspeed_zero2.yaml \
    examples/llada/sft.py \
    --num_train_epochs 4
```
```shell
# Submit to a Slurm cluster (DeepSpeed ZeRO-2 with 8 GPUs)
sbatch --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "deepspeed_zero2" \
    --script_path "examples/llada/sft.py" \
    --num_train_epochs 4
# Submit to a Slurm cluster (DeepSpeed ZeRO-2 with 2 nodes, 16 GPUs)
sbatch --nodes=2 --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "deepspeed_zero2" \
    --script_path "examples/llada/sft.py" \
    --num_train_epochs 4
```
See [Features & Documentation](#features--documentations) for training/inference details and task-specific recipes.


## Roadmap

- [ ] Support for additional diffusion LLMs.  

- [ ] Support for evaluation.

- [ ] Support for RL finetuning.


## Citation
```
@misc{dllm,
    author = {Zhanhui Zhou and Lingjie Chen},
    title = {dLLM: Training Diffusion Large Language Models Made Simple},
    howpublished = {https://github.com/ZHZisZZ/dllm},
    note = {Accessed: 2025-10-12},
    year = {2025}
}
```
